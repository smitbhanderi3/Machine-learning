{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Lecture 12: Feature importances\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import sys\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"code1/.\")\n",
    "\n",
    "import seaborn as sns\n",
    "#from plotting_functions import *\n",
    "from sklearn import datasets\n",
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "from sklearn.dummy import DummyClassifier, DummyRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.model_selection import (\n",
    "    GridSearchCV,\n",
    "    RandomizedSearchCV,\n",
    "    cross_val_score,\n",
    "    cross_validate,\n",
    "    train_test_split,\n",
    ")\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Learning outcomes \n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Interpret the coefficients of linear regression for ordinal, one-hot encoded categorical, and scaled numeric features. \n",
    "- Explain why interpretability is important in ML.\n",
    "- Use `feature_importances_` attribute of `sklearn` models and interpret its output. \n",
    "- Use `eli5` to get feature importances of non `sklearn` models and interpret its output. \n",
    "- Apply SHAP to assess feature importances and interpret model predictions. \n",
    "- Explain force plot, summary plot, and dependence plot produced with shapely values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data \n",
    "\n",
    "In this lecture, we'll be using [Kaggle House Prices dataset](https://www.kaggle.com/c/home-data-for-ml-course/), the dataset we used in lecture 2. As usual, to run this notebook you'll need to download the data. Unzip the data into a subdirectory called `data`. For this dataset, train and test have already been separated. We'll be working with the train portion in this lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>303</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>118.0</td>\n",
       "      <td>13704</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>205000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>767</th>\n",
       "      <td>768</td>\n",
       "      <td>50</td>\n",
       "      <td>RL</td>\n",
       "      <td>75.0</td>\n",
       "      <td>12508</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Shed</td>\n",
       "      <td>1300</td>\n",
       "      <td>7</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>160000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>430</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>130.0</td>\n",
       "      <td>11457</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2009</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1140</td>\n",
       "      <td>30</td>\n",
       "      <td>RL</td>\n",
       "      <td>98.0</td>\n",
       "      <td>8731</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>558</th>\n",
       "      <td>559</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>57.0</td>\n",
       "      <td>21872</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR2</td>\n",
       "      <td>HLS</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>175000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "302    303          20       RL        118.0    13704   Pave   NaN      IR1   \n",
       "767    768          50       RL         75.0    12508   Pave   NaN      IR1   \n",
       "429    430          20       RL        130.0    11457   Pave   NaN      IR1   \n",
       "1139  1140          30       RL         98.0     8731   Pave   NaN      IR1   \n",
       "558    559          60       RL         57.0    21872   Pave   NaN      IR2   \n",
       "\n",
       "     LandContour Utilities  ... PoolArea PoolQC Fence MiscFeature MiscVal  \\\n",
       "302          Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "767          Lvl    AllPub  ...        0    NaN   NaN        Shed    1300   \n",
       "429          Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "1139         Lvl    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "558          HLS    AllPub  ...        0    NaN   NaN         NaN       0   \n",
       "\n",
       "     MoSold YrSold  SaleType  SaleCondition  SalePrice  \n",
       "302       1   2006        WD         Normal     205000  \n",
       "767       7   2008        WD         Normal     160000  \n",
       "429       3   2009        WD         Normal     175000  \n",
       "1139      5   2007        WD         Normal     144000  \n",
       "558       8   2008        WD         Normal     175000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/housing-kaggle/train.csv\")\n",
    "train_df, test_df = train_test_split(df, test_size=0.10, random_state=123)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- The prediction task is predicting `SalePrice` given features related to properties.  \n",
    "- Note that the target is numeric, not categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1314, 81)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Let's separate `X` and `y`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df.drop(columns=[\"SalePrice\"])\n",
    "y_train = train_df[\"SalePrice\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"SalePrice\"])\n",
    "y_test = test_df[\"SalePrice\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Let's identify feature types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "drop_features = [\"Id\"]\n",
    "numeric_features = [\n",
    "    \"BedroomAbvGr\",\n",
    "    \"KitchenAbvGr\",\n",
    "    \"LotFrontage\",\n",
    "    \"LotArea\",\n",
    "    \"OverallQual\",\n",
    "    \"OverallCond\",\n",
    "    \"YearBuilt\",\n",
    "    \"YearRemodAdd\",\n",
    "    \"MasVnrArea\",\n",
    "    \"BsmtFinSF1\",\n",
    "    \"BsmtFinSF2\",\n",
    "    \"BsmtUnfSF\",\n",
    "    \"TotalBsmtSF\",\n",
    "    \"1stFlrSF\",\n",
    "    \"2ndFlrSF\",\n",
    "    \"LowQualFinSF\",\n",
    "    \"GrLivArea\",\n",
    "    \"BsmtFullBath\",\n",
    "    \"BsmtHalfBath\",\n",
    "    \"FullBath\",\n",
    "    \"HalfBath\",\n",
    "    \"TotRmsAbvGrd\",\n",
    "    \"Fireplaces\",\n",
    "    \"GarageYrBlt\",\n",
    "    \"GarageCars\",\n",
    "    \"GarageArea\",\n",
    "    \"WoodDeckSF\",\n",
    "    \"OpenPorchSF\",\n",
    "    \"EnclosedPorch\",\n",
    "    \"3SsnPorch\",\n",
    "    \"ScreenPorch\",\n",
    "    \"PoolArea\",\n",
    "    \"MiscVal\",\n",
    "    \"YrSold\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       " ['Po', 'Fa', 'TA', 'Gd', 'Ex']]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ordinal_features_reg = [\n",
    "    \"ExterQual\",\n",
    "    \"ExterCond\",\n",
    "    \"BsmtQual\",\n",
    "    \"BsmtCond\",\n",
    "    \"HeatingQC\",\n",
    "    \"KitchenQual\",\n",
    "    \"FireplaceQu\",\n",
    "    \"GarageQual\",\n",
    "    \"GarageCond\",\n",
    "    \"PoolQC\",\n",
    "]\n",
    "ordering = [\n",
    "    \"Po\",\n",
    "    \"Fa\",\n",
    "    \"TA\",\n",
    "    \"Gd\",\n",
    "    \"Ex\",\n",
    "]  # if N/A it will just impute something, per below\n",
    "ordering_ordinal_reg = [ordering] * len(ordinal_features_reg)\n",
    "ordering_ordinal_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ordinal_features_oth = [\n",
    "    \"BsmtExposure\",\n",
    "    \"BsmtFinType1\",\n",
    "    \"BsmtFinType2\",\n",
    "    \"Functional\",\n",
    "    \"Fence\",\n",
    "]\n",
    "ordering_ordinal_oth = [\n",
    "    [\"NA\", \"No\", \"Mn\", \"Av\", \"Gd\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"NA\", \"Unf\", \"LwQ\", \"Rec\", \"BLQ\", \"ALQ\", \"GLQ\"],\n",
    "    [\"Sal\", \"Sev\", \"Maj2\", \"Maj1\", \"Mod\", \"Min2\", \"Min1\", \"Typ\"],\n",
    "    [\"NA\", \"MnWw\", \"GdWo\", \"MnPrv\", \"GdPrv\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SaleType',\n",
       " 'HouseStyle',\n",
       " 'Heating',\n",
       " 'SaleCondition',\n",
       " 'MSZoning',\n",
       " 'Street',\n",
       " 'RoofMatl',\n",
       " 'Foundation',\n",
       " 'Exterior2nd',\n",
       " 'Alley',\n",
       " 'Exterior1st',\n",
       " 'MasVnrType',\n",
       " 'LandSlope',\n",
       " 'GarageFinish',\n",
       " 'PavedDrive',\n",
       " 'LandContour',\n",
       " 'Utilities',\n",
       " 'RoofStyle',\n",
       " 'LotConfig',\n",
       " 'MoSold',\n",
       " 'LotShape',\n",
       " 'Condition2',\n",
       " 'Condition1',\n",
       " 'MiscFeature',\n",
       " 'Electrical',\n",
       " 'BldgType',\n",
       " 'MSSubClass',\n",
       " 'CentralAir',\n",
       " 'GarageType',\n",
       " 'Neighborhood']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_features = list(\n",
    "    set(X_train.columns)\n",
    "    - set(numeric_features)\n",
    "    - set(ordinal_features_reg)\n",
    "    - set(ordinal_features_oth)\n",
    "    - set(drop_features)\n",
    ")\n",
    "categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_transformer\n",
    "\n",
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "ordinal_transformer_reg = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_reg),\n",
    ")\n",
    "\n",
    "ordinal_transformer_oth = make_pipeline(\n",
    "    SimpleImputer(strategy=\"most_frequent\"),\n",
    "    OrdinalEncoder(categories=ordering_ordinal_oth),\n",
    ")\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\", sparse=False),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer_reg, ordinal_features_reg),\n",
    "    (ordinal_transformer_oth, ordinal_features_oth),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'drop': 'drop',\n",
       " 'pipeline-1': Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='median')),\n",
       "                 ('standardscaler', StandardScaler())]),\n",
       " 'pipeline-2': Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='most_frequent')),\n",
       "                 ('ordinalencoder',\n",
       "                  OrdinalEncoder(categories=[['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex'],\n",
       "                                             ['Po', 'Fa', 'TA', 'Gd', 'Ex']]))]),\n",
       " 'pipeline-3': Pipeline(steps=[('simpleimputer', SimpleImputer(strategy='most_frequent')),\n",
       "                 ('ordinalencoder',\n",
       "                  OrdinalEncoder(categories=[['NA', 'No', 'Mn', 'Av', 'Gd'],\n",
       "                                             ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ',\n",
       "                                              'ALQ', 'GLQ'],\n",
       "                                             ['NA', 'Unf', 'LwQ', 'Rec', 'BLQ',\n",
       "                                              'ALQ', 'GLQ'],\n",
       "                                             ['Sal', 'Sev', 'Maj2', 'Maj1',\n",
       "                                              'Mod', 'Min2', 'Min1', 'Typ'],\n",
       "                                             ['NA', 'MnWw', 'GdWo', 'MnPrv',\n",
       "                                              'GdPrv']]))]),\n",
       " 'pipeline-4': Pipeline(steps=[('simpleimputer',\n",
       "                  SimpleImputer(fill_value='missing', strategy='constant')),\n",
       "                 ('onehotencoder',\n",
       "                  OneHotEncoder(handle_unknown='ignore', sparse=False,\n",
       "                                sparse_output=False))])}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessor.fit(X_train)\n",
    "preprocessor.named_transformers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OneHotEncoder' object has no attribute 'get_feature_names'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m ohe_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_transformers_\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpipeline-4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_steps\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43monehotencoder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_feature_names\u001b[49m(categorical_features)\n\u001b[0;32m      5\u001b[0m )\n\u001b[0;32m      6\u001b[0m new_columns \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m      7\u001b[0m     numeric_features \u001b[38;5;241m+\u001b[39m ordinal_features_reg \u001b[38;5;241m+\u001b[39m ordinal_features_oth \u001b[38;5;241m+\u001b[39m ohe_columns\n\u001b[0;32m      8\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OneHotEncoder' object has no attribute 'get_feature_names'"
     ]
    }
   ],
   "source": [
    "ohe_columns = list(\n",
    "    preprocessor.named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names(categorical_features)\n",
    ")\n",
    "new_columns = (\n",
    "    numeric_features + ordinal_features_reg + ordinal_features_oth + ohe_columns\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_columns' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [22]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train_enc \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m----> 2\u001b[0m     preprocessor\u001b[38;5;241m.\u001b[39mtransform(X_train), index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39m\u001b[43mnew_columns\u001b[49m\n\u001b[0;32m      3\u001b[0m )\n\u001b[0;32m      4\u001b[0m X_train_enc\n",
      "\u001b[1;31mNameError\u001b[0m: name 'new_columns' is not defined"
     ]
    }
   ],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    preprocessor.transform(X_train), index=X_train.index, columns=new_columns\n",
    ")\n",
    "X_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's look at the correlations between various features with other features and the target in our encoded data (first row/column). \n",
    "- In simple terms here is how you can interpret correlations between two variables $X$ and $Y$:\n",
    "  - If $Y$ goes up when $X$ goes up, we say $X$ and $Y$ are positively correlated.\n",
    "  - If $Y$ goes down when $X$ goes up, we say $X$ and $Y$ are negatively correlated.\n",
    "  - If $Y$ is unchanged when $X$ changes, we say $X$ and $Y$ are uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cor = pd.concat((y_train, X_train_enc), axis=1).iloc[:, :15].corr()\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can immediately see that `SalePrice` is highly correlated with `OverallQual`.\n",
    "- This is an early hint that `OverallQual` is a useful feature in predicting `SalePrice`.\n",
    "- However, this approach is **extremely simplistic**.\n",
    "  - It only looks at each feature in isolation.\n",
    "  - It only looks at linear associations:\n",
    "    - What if `SalePrice` is high when `BsmtFullBath` is 2 or 3, but low when it's 0, 1, or 4? They might seem uncorrelated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cor = pd.concat((y_train, X_train_enc), axis=1).iloc[:, 10:15].corr()\n",
    "plt.figure(figsize=(5, 5))\n",
    "sns.set(font_scale=1)\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Looking at this diagram also tells us the relationship between features. \n",
    "  - For example, `1stFlrSF` and `TotalBsmtSF` are highly correlated. \n",
    "  - Do we need both of them?\n",
    "  - If our model says `1stFlrSF` is very important and `TotalBsmtSF` is very unimportant, do we trust those values?\n",
    "  - Maybe `TotalBsmtSF` only \"becomes important\" if `1stFlrSF` is removed.\n",
    "  - Sometimes the opposite happens: a feature only becomes important if another feature is _added_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Feature importances in linear models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Like logistic regression, with linear regression we can look at the _coefficients_ for each feature.\n",
    "- Overall idea: predicted price = intercept + $\\sum_i$ coefficient i $\\times$ feature i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = make_pipeline(preprocessor, Ridge())\n",
    "lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the coefficients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs = pd.DataFrame(data=lr[1].coef_, index=new_columns, columns=[\"Coefficient\"])\n",
    "lr_coefs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting coefficients of different types of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Ordinal features\n",
    "\n",
    "- The ordinal features are easiest to interpret. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ordinal_features_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"ExterQual\", \"Coefficient\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"ExterCond\", \"Coefficient\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Increasing by one category of exterior quality (e.g. good -> excellent) increases the predicted price by $\\sim\\$4195$.\n",
    "  - Wow, that's a lot! \n",
    "  - Remember this is just what the model has learned. It doesn't tell us how the world works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "one_example = X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example[\"ExterQual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's perturb the example and change `ExterQual` to `Ex`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed = one_example.copy()\n",
    "one_example_perturbed[\"ExterQual\"] = \"Ex\"  # Change Gd to Ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "one_example_perturbed[\"ExterQual\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the prediction change after changing `ExterQual` from `Gd` to `Ex`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Prediction on the original example: \", lr.predict(one_example))\n",
    "print(\"Prediction on the perturbed example: \", lr.predict(one_example_perturbed))\n",
    "print(\n",
    "    \"After changing ExterQual from Gd to Ex increased the prediction by: \",\n",
    "    lr.predict(one_example_perturbed) - lr.predict(one_example),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's exactly the learned coefficient for `ExterQual`! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"ExterQual\", \"Coefficient\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our interpretation is correct! \n",
    "- Increasing by one category of exterior quality (e.g. good -> excellent) increases the predicted price by $\\sim\\$4195$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "- What about the categorical features?\n",
    "- We have created a number of columns for each category with OHE and each category gets it's own coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs_landslope = lr_coefs[lr_coefs.index.str.startswith(\"LandSlope\")]\n",
    "lr_coefs_landslope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We can talk about switching from one of these categories to another by picking a \"reference\" category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs_landslope - lr_coefs_landslope.loc[\"LandSlope_Gtl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you change the category from `LandSlope_Gtl` to `LandSlope_Mod` the prediction price goes up by $\\sim\\$6963$\n",
    "- If you change the category from `LandSlope_Gtl` to `LandSlope_Sev` the prediction price goes down by $\\sim\\$8334$\n",
    "\n",
    "\n",
    "Note that this might not make sense in the real world but this is what our model decided to learn given this small amount of data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_coefs.sort_values(by=\"Coefficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For example, the above coefficient says that \"If the roof is made of clay or tile, the predicted price is \\\\$191K less\"?\n",
    "- Do we believe these interpretations??\n",
    "  - Do we believe this is how the predictions are being computed? Yes.\n",
    "  - Do we believe that this is how the world works? No. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "If you did `drop='first'` (we didn't) then you already have a reference class, and all the values are with respect to that one. The interpretation depends on whether we did `drop='first'`, hence the hassle.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting coefficients of numeric features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's look at coefficients of `PoolArea` and `LotFrontage`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"PoolArea\", \"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition: \n",
    "\n",
    "- Tricky because numeric features are **scaled**! \n",
    "- **Increasing** `PoolArea` by 1 scaled unit **increases** the predicted price by $\\sim\\$2822$.\n",
    "- **Increasing** `LotFrontage` by 1 scaled unit **decreases** the predicted price by $\\sim\\$1578$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Does that sound reasonable?\n",
    "\n",
    "- For `PoolArea`, yes. \n",
    "- For `LotFrontage`, that's surprising. Something positive would have made more sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "It's not the case here but maybe the problem is that `LotFrontage` and `LotArea` are very correlated. `LotArea` has a larger positive coefficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "cor = X_train_enc[numeric_features[:5]].corr()\n",
    "sns.heatmap(cor, annot=True, cmap=plt.cm.Blues);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "BTW, let's make sure the predictions behave as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example = X_test[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's perturb the example and add 1 to the `LotArea`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed = one_example.copy()\n",
    "one_example_perturbed[\"LotArea\"] += 1  # add 1 to the LotArea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_example_perturbed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prediction on the original example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.predict(one_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction on the perturbed example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr.predict(one_example_perturbed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- What's the difference between prediction? \n",
    "- Does the difference make sense given the coefficient of the feature? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr.predict(one_example_perturbed) - lr.predict(one_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Why did the prediction only go up by \\\\$0.57 instead of \\\\$5109? \n",
    "- This is an issue of units. `LotArea` is in sqft, but the coefficient is not $\\$5109/\\text{sqft}$ **because we scaled the features**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example showing how can we interpret coefficients of scaled features. \n",
    "\n",
    "- The scaler subtracted the mean and divided by the standard deviation.\n",
    "- The division actually changed the scale! \n",
    "- For the unit conversion, we don't care about the subtraction, but only the scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scaler = preprocessor.named_transformers_[\"pipeline-1\"][\"standardscaler\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.sqrt(scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_scales = pd.DataFrame(\n",
    "    data=np.sqrt(scaler.var_), index=numeric_features, columns=[\"Scale\"]\n",
    ")\n",
    "lr_scales.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems like `LotArea` was divided by 8994.471032 sqft. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"LotArea\", \"Coefficient\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[\"LotArea\", \"Coefficient\"] / lr_scales.loc[\"LotArea\", \"Scale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "lr_coefs.loc[[\"LotArea\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- The coefficient tells us that if we increase the **scaled** `LotArea` by one unit the price would go up by $\\approx\\$5109$. \n",
    "- One scaled unit represents $\\sim 8994$ sq feet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- So if I increase original `LotArea` by one square foot then the predicted price would go up by this amount: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "5109.356718094072 / 8994.471032"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- This makes much more sense. Now we get the number we got before. \n",
    "- That said don't read too much into these coefficients without statistical training. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary\n",
    "\n",
    "- Correlation among features might make coefficients completely uninterpretable. \n",
    "- Fairly straightforward to interpret coefficients of ordinal features. \n",
    "- In categorical features, it's often helpful to consider one category as a reference point and think about relative importance. \n",
    "- For numeric features, relative importance is meaningful after scaling.\n",
    "- You have to be careful about the scale of the feature when interpreting the coefficients. \n",
    "- Remember that explaining the model $\\neq$ explaining the data.  \n",
    "- the coefficients tell us only about the model and they might not accurately reflect the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (5 min)\n",
    "\n",
    "![](img/eva-coffee.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Interpretability of ML models: Motivations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Why model interpretability? \n",
    "\n",
    "- Ability to interpret ML models is crucial in many applications such as banking, healthcare, and criminal justice.\n",
    "- It can be leveraged by domain experts to diagnose systematic errors and underlying biases of complex ML systems. \n",
    "\n",
    "![](img/shap_example.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_example.png\" width=\"600\" height=\"600\"> -->\n",
    "    \n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is model interpretability? \n",
    "\n",
    "- In this course, our definition of model iterpretability will be looking at **feature importances**.\n",
    "- There is more to interpretability than feature importances, but it's a good start!\n",
    "- Resource: \n",
    "    - [Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/interpretability-importance.html)\n",
    "    - [Yann LeCun, Kilian Weinberger, Patrice Simard, and Rich Caruana: Panel debate on interpretability](https://vimeo.com/252187813)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data\n",
    "\n",
    "- Let's work with [the adult census data set](https://www.kaggle.com/uciml/adult-census-income) from last lecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "adult_df_large = pd.read_csv(\"data/adult.csv\")\n",
    "train_df, test_df = train_test_split(adult_df_large, test_size=0.2, random_state=42)\n",
    "train_df_nan = train_df.replace(\"?\", np.NaN)\n",
    "test_df_nan = test_df.replace(\"?\", np.NaN)\n",
    "train_df_nan.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = [\"age\", \"fnlwgt\", \"capital.gain\", \"capital.loss\", \"hours.per.week\"]\n",
    "categorical_features = [\n",
    "    \"workclass\",\n",
    "    \"marital.status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"native.country\",\n",
    "]\n",
    "ordinal_features = [\"education\"]\n",
    "binary_features = [\"sex\"]\n",
    "drop_features = [\"race\", \"education.num\"]\n",
    "target_column = \"income\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "education_levels = [\n",
    "    \"Preschool\",\n",
    "    \"1st-4th\",\n",
    "    \"5th-6th\",\n",
    "    \"7th-8th\",\n",
    "    \"9th\",\n",
    "    \"10th\",\n",
    "    \"11th\",\n",
    "    \"12th\",\n",
    "    \"HS-grad\",\n",
    "    \"Prof-school\",\n",
    "    \"Assoc-voc\",\n",
    "    \"Assoc-acdm\",\n",
    "    \"Some-college\",\n",
    "    \"Bachelors\",\n",
    "    \"Masters\",\n",
    "    \"Doctorate\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert set(education_levels) == set(train_df[\"education\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"), StandardScaler())\n",
    "tree_numeric_transformer = make_pipeline(SimpleImputer(strategy=\"median\"))\n",
    "\n",
    "categorical_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(handle_unknown=\"ignore\"),\n",
    ")\n",
    "\n",
    "ordinal_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OrdinalEncoder(categories=[education_levels], dtype=int),\n",
    ")\n",
    "\n",
    "binary_transformer = make_pipeline(\n",
    "    SimpleImputer(strategy=\"constant\", fill_value=\"missing\"),\n",
    "    OneHotEncoder(drop=\"if_binary\", dtype=int),\n",
    ")\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (\"drop\", drop_features),\n",
    "    (numeric_transformer, numeric_features),\n",
    "    (ordinal_transformer, ordinal_features),\n",
    "    (binary_transformer, binary_features),\n",
    "    (categorical_transformer, categorical_features),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train = train_df_nan.drop(columns=[target_column])\n",
    "y_train = train_df_nan[target_column]\n",
    "\n",
    "X_test = test_df_nan.drop(columns=[target_column])\n",
    "y_test = test_df_nan[target_column]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do we have class imbalance? \n",
    "\n",
    "- There is class imbalance. But without any context, both classes seem equally important. \n",
    "- Let's use accuracy as our metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_df_nan[\"income\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "scoring_metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's store all the results in a dictionary called `results`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "scoring_metric = \"accuracy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe_lr = make_pipeline(\n",
    "    preprocessor, LogisticRegression(max_iter=2000, random_state=123)\n",
    ")\n",
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))\n",
    "pipe_xgb = make_pipeline(\n",
    "    preprocessor, XGBClassifier(random_state=123, eval_metric=\"logloss\", verbosity=0)\n",
    ")\n",
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "classifiers = {\n",
    "    \"logistic regression\": pipe_lr,\n",
    "    \"random forest\": pipe_rf,\n",
    "    \"XGBoost\": pipe_xgb,\n",
    "    \"LightGBM\": pipe_lgbm,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model :\n",
    "        scikit-learn model\n",
    "    X_train : numpy array or pandas DataFrame\n",
    "        X in the training data\n",
    "    y_train :\n",
    "        y in the training data\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "        pandas Series with mean scores from cross_validation\n",
    "    \"\"\"\n",
    "\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "dummy = DummyClassifier(strategy=\"stratified\")\n",
    "results[\"Dummy\"] = mean_std_cross_val_scores(\n",
    "    dummy, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for (name, model) in classifiers.items():\n",
    "    results[name] = mean_std_cross_val_scores(\n",
    "        model, X_train, y_train, return_train_score=True, scoring=scoring_metric\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- One problem is that often simple models are interpretable but not accurate.\n",
    "- But more complex models (e.g., LightGBM) are less interpretable.\n",
    "\n",
    "![](img/shap_motivation.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_motivation.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature importances in linear models\n",
    "\n",
    "- Simpler models are often more interpretable but less accurate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create and fit a pipeline with preprocessor and logistic regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=2000, random_state=2))\n",
    "pipe_lr.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ohe_feature_names = (\n",
    "    pipe_rf.named_steps[\"columntransformer\"]\n",
    "    .named_transformers_[\"pipeline-4\"]\n",
    "    .named_steps[\"onehotencoder\"]\n",
    "    .get_feature_names()\n",
    "    .tolist()\n",
    ")\n",
    "feature_names = (\n",
    "    numeric_features + ordinal_features + binary_features + ohe_feature_names\n",
    ")\n",
    "feature_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"coefficient\": pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist(),\n",
    "    \"magnitude\": np.absolute(\n",
    "        pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist()\n",
    "    ),\n",
    "}\n",
    "coef_df = pd.DataFrame(data, index=feature_names).sort_values(\n",
    "    \"magnitude\", ascending=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "coef_df[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Increasing `capital.gain` is likely to push the prediction towards \">50k\" income class \n",
    "- Whereas occupation of private house service is likely to push the prediction towards \"<=50K\" income. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can we get feature importances for non-linear models? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Model interpretability beyond linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will be looking at three ways for model interpretability. \n",
    "\n",
    "- `sklearn` `feature_importances_`    \n",
    "- [eli5](https://eli5.readthedocs.io/en/latest/tutorials/black-box-text-classifiers.html#lime-tutorial) (stands for \"explain like I'm 5\") \n",
    "- [SHAP](https://github.com/slundberg/shap)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `sklearn` `feature_importances_`\n",
    "\n",
    "- Many `sklearn` models have `feature_importances_` attribute.\n",
    "- For tree-based models it's calculated based on impurity (gini index or information gain).\n",
    "- For example, let's look at `feature_importances_` of `RandomForestClassifier`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create and fit a pipeline with preprocessor and random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=2))\n",
    "pipe_rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which features are driving the predictions the most? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Importance\": pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_,\n",
    "}\n",
    "imps = pd.DataFrame(data=data, index=feature_names,).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")[:10]\n",
    "imps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Key point \n",
    "\n",
    "- Unlike the linear model coefficients, `feature_importances_` **do not have a sign**!\n",
    "  - They tell us about importance, but not an \"up or down\".\n",
    "  - Indeed, increasing a feature may cause the prediction to first go up, and then go down.\n",
    "  - This cannot happen in linear models, because they are linear. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Do these importances match with importances identified by logistic regression? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"random forest importance\": pipe_rf.named_steps[\n",
    "        \"randomforestclassifier\"\n",
    "    ].feature_importances_,\n",
    "    \"logistic regression importances\": pipe_lr.named_steps[\"logisticregression\"]\n",
    "    .coef_[0]\n",
    "    .tolist(),\n",
    "}\n",
    "imps = pd.DataFrame(\n",
    "    data=data,\n",
    "    index=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "imps.sort_values(by=\"random forest importance\", ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Both models don't agree on `age`, `education`, `capital.gain`\n",
    "- The actual numbers for random forests and logistic regression are not really comparable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we get feature importances for non `sklearn` models? \n",
    "\n",
    "- One way to do it is by using a tool called [`eli5`](https://eli5.readthedocs.io/en/latest/overview.html).\n",
    "\n",
    "You'll have to install it\n",
    "\n",
    "```\n",
    "conda install -c conda-forge eli5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at feature importances for `XGBClassifier`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{note}\n",
    "For some reason, `eli5.explain_weights` doesn't work with the latest version of `xgboost`. I downgraded the `xgboost` version using this command to make it work `conda install -c conda-forge py-xgboost==1.3.0`. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge py-xgboost==1.3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c conda-forge eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting SHAP\n",
      "  Downloading shap-0.41.0-cp39-cp39-win_amd64.whl (435 kB)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (21.3)\n",
      "Requirement already satisfied: tqdm>4.25.0 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (4.64.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (1.21.5)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (1.2.1)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (2.0.0)\n",
      "Requirement already satisfied: numba in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (0.55.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (1.7.3)\n",
      "Requirement already satisfied: pandas in c:\\users\\smit\\anaconda3\\lib\\site-packages (from SHAP) (1.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from packaging>20.9->SHAP) (3.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\smit\\anaconda3\\lib\\site-packages (from tqdm>4.25.0->SHAP) (0.4.4)\n",
      "Requirement already satisfied: llvmlite<0.39,>=0.38.0rc1 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from numba->SHAP) (0.38.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\smit\\anaconda3\\lib\\site-packages (from numba->SHAP) (61.2.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from pandas->SHAP) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from pandas->SHAP) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->SHAP) (1.16.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from scikit-learn->SHAP) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\smit\\anaconda3\\lib\\site-packages (from scikit-learn->SHAP) (2.2.0)\n",
      "Installing collected packages: slicer, SHAP\n",
      "Successfully installed SHAP-0.41.0 slicer-0.0.7\n"
     ]
    }
   ],
   "source": [
    "!pip install SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01meli5\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m pipe_xgb \u001b[38;5;241m=\u001b[39m \u001b[43mmake_pipeline\u001b[49m(preprocessor, XGBClassifier(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m123\u001b[39m, eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m      4\u001b[0m pipe_xgb\u001b[38;5;241m.\u001b[39mfit(X_train, y_train);\n\u001b[0;32m      5\u001b[0m eli5\u001b[38;5;241m.\u001b[39mexplain_weights(pipe_xgb\u001b[38;5;241m.\u001b[39mnamed_steps[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxgbclassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m], feature_names\u001b[38;5;241m=\u001b[39mfeature_names)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "import eli5\n",
    "\n",
    "pipe_xgb = make_pipeline(preprocessor, XGBClassifier(random_state=123, eval_metric=\"logloss\", verbosity=0))\n",
    "pipe_xgb.fit(X_train, y_train);\n",
    "eli5.explain_weights(pipe_xgb.named_steps[\"xgbclassifier\"], feature_names=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at feature importances for `LGBMClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\n",
    "pipe_lgbm.fit(X_train, y_train)\n",
    "# eli5.explain_weights(\n",
    "#     pipe_lgbm.named_steps[\"lgbmclassifier\"], feature_names=feature_names\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You can also look at feature importances for `RandomForestClassifier`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5.explain_weights(\n",
    "    pipe_rf.named_steps[\"randomforestclassifier\"], feature_names=feature_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare them with weights what we got with `sklearn` `feature_importances_`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"Importance\": pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_,\n",
    "}\n",
    "pd.DataFrame(data=data, index=feature_names,).sort_values(\n",
    "    by=\"Importance\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- These values tell us globally about which features are important.\n",
    "- But what if you want to explain a _specific_ prediction. \n",
    "- Some fancier tools can help us do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SHAP  (SHapley Additive exPlanations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### SHAP  (SHapley Additive exPlanations)\n",
    "- A sophisticated measure of the contribution of each feature.\n",
    "- [Lundberg and Lee, 2017](https://arxiv.org/pdf/1705.07874.pdf)\n",
    "- We won't go in details. You may refer to [Scott Lundberg's GitHub repo](https://github.com/slundberg/shap) if you are interested to know more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea\n",
    "\n",
    "![](img/shap_example.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_example.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea\n",
    "\n",
    "- Provides following kind of explanation\n",
    "    - Start at a base rate (e.g., how often people get their loans rejected).\n",
    "    - Add one feature at a time and see how it impacts the decision. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![](img/shap_explanation2.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_explanation2.png\" width=\"1000\" height=\"1000\"> -->\n",
    "\n",
    "[Source](https://github.com/slundberg/shap/blob/master/docs/presentations/February%202018%20Talk.pptx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "Let's try it out on tree-based models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "First you'll have to install it. \n",
    "\n",
    "```\n",
    "pip install shap\n",
    "or\n",
    "conda install -c conda-forge shap\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's create train and test dataframes with our transformed features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_train).toarray(),\n",
    "    columns=feature_names,\n",
    "    index=X_train.index,\n",
    ")\n",
    "X_train_enc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_test_enc = pd.DataFrame(\n",
    "    data=preprocessor.transform(X_test).toarray(),\n",
    "    columns=feature_names,\n",
    "    index=X_test.index,\n",
    ")\n",
    "\n",
    "X_test_enc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's get SHAP values for train and test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "lgbm_explainer = shap.TreeExplainer(pipe_lgbm.named_steps[\"lgbmclassifier\"])\n",
    "train_lgbm_shap_values = lgbm_explainer.shap_values(X_train_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lgbm_shap_values = lgbm_explainer.shap_values(X_test_enc)\n",
    "test_lgbm_shap_values[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For classification it's a bit confusing. It gives SHAP arrays both classes.\n",
    "- Let's stick to shap values for class 1, i.e., income > 50K. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example and each feature we have a SHAP value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lgbm_shap_values[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the average SHAP values associated with each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.abs(train_lgbm_shap_values[1]).mean(0)\n",
    "pd.DataFrame(data=values, index=feature_names, columns=[\"SHAP\"]).sort_values(\n",
    "    by=\"SHAP\", ascending=False\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can think of this as global feature importances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SHAP plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# load JS visualization code to notebook\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dependence plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"age\", train_lgbm_shap_values[1], X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot above shows effect of `age` feature on the prediction. \n",
    "\n",
    "- Each dot is a single prediction for examples above.\n",
    "- The x-axis represents values of the feature age (scaled).\n",
    "- The y-axis is the SHAP value for that feature, which represents how much knowing that feature's value changes the output of the model for that example's prediction. \n",
    "- Lower values of age have smaller SHAP values for class \">50K\".\n",
    "- Similarly, higher values of age also have a bit smaller SHAP values for class \">50K\", which makes sense.  \n",
    "- There is some optimal value of age between scaled age of 1 which gives highest SHAP values for for class \">50K\". \n",
    "- Ignore the colour for now. The color corresponds to a second feature (education feature in this case) that may have an interaction effect with the feature we are plotting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "shap.summary_plot(train_lgbm_shap_values[1], X_train_enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The plot shows the most important features for predicting the class. It also shows the direction of how it's going to drive the prediction.  \n",
    "\n",
    "- Presence of the marital status of Married-civ-spouse seems to have bigger SHAP values for class 1 and absence seems to have smaller SHAP values for class 1. \n",
    "- Higher levels of education seem to have bigger SHAP values for class 1 whereas smaller levels of education have smaller SHAP values. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Force plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's try to explain predictions on a couple of examples from the test data. \n",
    "- I'm sampling some examples where target is <=50K and some examples where target is >50K. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_test_reset = y_test.reset_index(drop=True)\n",
    "y_test_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "l50k_ind = y_test_reset[y_test_reset == \"<=50K\"].index.tolist()\n",
    "g50k_ind = y_test_reset[y_test_reset == \">50K\"].index.tolist()\n",
    "\n",
    "ex_l50k_index = l50k_ind[10]\n",
    "ex_g50k_index = g50k_ind[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_g50k_index, ex_l50k_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_enc.iloc[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example with prediction <=50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict_proba(X_test_enc)[ex_l50k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)[\n",
    "    ex_l50k_index\n",
    "]  # raw score of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][ex_l50k_index, :],\n",
    "    X_test_enc.iloc[ex_l50k_index, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example with prediction >50K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict_proba(X_test_enc)[ex_g50k_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pipe_lgbm.named_steps[\"lgbmclassifier\"].predict(X_test_enc, raw_score=True)[\n",
    "    ex_g50k_index\n",
    "]  # raw model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g50k_ind[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][18, :],\n",
    "    X_test_enc.iloc[18, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lgbm_shap_values[1][ex_g50k_index, :],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(\n",
    "    lgbm_explainer.expected_value[1],\n",
    "    test_lgbm_shap_values[1][ex_g50k_index, :],\n",
    "    X_test_enc.iloc[ex_g50k_index, :],\n",
    "    matplotlib=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observations: \n",
    "\n",
    "- Everything is with respect to class 1 here. \n",
    "- The base value for class 1 is -2.316. (You can think of this as the average raw score.) \n",
    "- We see the forces that drive the prediction. \n",
    "- That is, we can see the main factors pushing it from the base value (average over the dataset) to this particular prediction. \n",
    "- Features that push the prediction to a higher value are shown in red. \n",
    "- Features that push the prediction to a lower value are shown in blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note: a nice thing about SHAP values is that the feature importances sum to the prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lgbm_shap_values[1][ex_g50k_index, :].sum() + lgbm_explainer.expected_value[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Provides explainer for different kinds of models\n",
    "\n",
    "- [TreeExplainer](https://shap.readthedocs.io/en/latest/) (supports XGBoost, CatBoost, LightGBM) \n",
    "- [DeepExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.DeepExplainer) (supports deep-learning models)\n",
    "- [KernelExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.KernelExplainer) (supports kernel-based models)\n",
    "- [GradientExplainer](https://shap.readthedocs.io/en/latest/index.html#shap.GradientExplainer) (supports Keras and Tensorflow models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Can also be used to explain text classification and image classification \n",
    "- Example: In the picture below, red pixels represent positive SHAP values that increase the probability of the class, while blue pixels represent negative SHAP values the reduce the probability of the class. \n",
    "\n",
    "![](img/shap_image_explainer.png)\n",
    "\n",
    "<!-- <img src=\"img/shap_image_explainer.png\" width=\"600\" height=\"600\"> -->\n",
    "\n",
    "[Source](https://github.com/slundberg/shap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Other tools\n",
    "\n",
    "- [lime](https://github.com/marcotcr/lime) is another package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're not already impressed, keep in mind:\n",
    "\n",
    "- So far we've only used sklearn models.\n",
    "- Most sklearn models have some built-in measure of feature importances.\n",
    "- On many tasks we need to move beyond sklearn, e.g. LightGBM, deep learning.\n",
    "- These tools work on other models as well, which makes them extremely useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Why do we want this information?\n",
    "\n",
    "Possible reasons:\n",
    "\n",
    "- Identify features that are not useful and maybe remove them.\n",
    "- Get guidance on what new data to collect.\n",
    "  - New features related to useful features -> better results.\n",
    "  - Don't bother collecting useless features -> save resources.\n",
    "- Help explain why the model is making certain predictions. \n",
    "  - Debugging, if the model is behaving strangely.\n",
    "  - Regulatory requirements.\n",
    "  - Fairness / bias.\n",
    "  - Keep in mind this can be used on **deployment** predictions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### â“â“ Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### True/False \n",
    "1. You train a random forest on a binary classification problem with two classes [neg, pos]. A value of 0.580 for feat1 given by `feature_importances_` attribute of your model means that increasing the value of feat1 will drive us towards positive class. \n",
    "2. eli5 can be used to get feature importances for non `sklearn` models. \n",
    "3. With SHAP you can only explain predictions on the training examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
