{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 3: Machine Learning Fundamentals \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# import the libraries\n",
    "#import os\n",
    "#import sys\n",
    "\n",
    "#import graphviz\n",
    "#import IPython\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import HTML\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#sys.path.append(\"code/.\")\n",
    "#from plotting_functions import *\n",
    "\n",
    "# Classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "#from utils import *\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning outcomes\n",
    "\n",
    "From this lecture, you will be able to \n",
    "\n",
    "- explain how decision boundaries change with the `max_depth` hyperparameter;\n",
    "- explain the concept of generalization;\n",
    "- appropriately split a dataset into train and test sets using `train_test_split` function;\n",
    "- explain the difference between train, validation, test, and \"deployment\" data;\n",
    "- identify the difference between training error, validation error, and test error;\n",
    "- explain cross-validation and use `cross_val_score` and `cross_validate` to calculate cross-validation error;\n",
    "- recognize overfitting and/or underfitting by looking at train and test scores;\n",
    "- explain why it is generally not possible to get a perfect test score (zero test error) on a supervised learning problem;\n",
    "- describe the fundamental tradeoff between training score and the train-test gap;\n",
    "- state the golden rule;\n",
    "- start to build a standard recipe for supervised learning: train/test split, hyperparameter tuning with cross-validation, test on test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Big picture and motivation \n",
    "\n",
    "In machine learning, we want to glean information from labeled data so that we can label **new unlabeled** data. For example, suppose we want to build a spam filtering system.  We will take a large number of spam/non-spam messages from the past, learn patterns associated with spam/non-spam from them, and predict whether **a new incoming message** in someone's inbox is spam or non-spam based on these patterns. \n",
    "\n",
    "So we want to learn from the past but ultimately we want to apply it on the future email messages. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/eva-think.png)\n",
    "\n",
    "**How can we generalize from what we've seen to what we haven't seen?** \n",
    "\n",
    "In this lecture, we'll see how machine learning tackles this question. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model complexity and training error\n",
    "\n",
    "In the last lecture, we looked at decision boundaries, a way to visualize what sort of examples will be classified as positive and negative. \n",
    "\n",
    "Let's examine how does the decision boundary change for different tree depths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Toy quiz2 grade data\n",
    "classification_df = pd.read_csv(\"data/quiz2-grade-toy-classification.csv\")\n",
    "classification_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = classification_df.drop([\"quiz2\"], axis=1)\n",
    "y = classification_df[\"quiz2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_subset = X[[\"lab4\", \"quiz1\"]]  # Let's consider a subset of the data for visualization\n",
    "X_subset.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following model, this decision boundary is created by asking one question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "depth = 1\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset, y)\n",
    "model.score(X_subset, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the following model, this decision boundary is created by asking two questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 2\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset, y)\n",
    "model.score(X_subset, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the decision boundary with depth = 4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "depth = 4\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset, y)\n",
    "model.score(X_subset, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at the decision boundary with depth = 6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "depth = 6\n",
    "model = DecisionTreeClassifier(max_depth=depth)\n",
    "model.fit(X_subset, y)\n",
    "model.score(X_subset, y)\n",
    "print(\"Error:   %0.3f\" % (1 - model.score(X_subset, y)))\n",
    "plot_tree_decision_boundary_and_tree(\n",
    "    model, X_subset, y, x_label=\"lab4\", y_label=\"quiz1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "max_depths = np.arange(1, 18)\n",
    "errors = []\n",
    "for max_depth in max_depths:\n",
    "    error = 1 - DecisionTreeClassifier(max_depth=max_depth).fit(X_subset, y).score(\n",
    "        X_subset, y\n",
    "    )\n",
    "    errors.append(error)\n",
    "plt.plot(max_depths, errors)\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"error\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Our model has 0% error for depths >= 6!! \n",
    "- But it's also becoming more and more specific and sensitive to the training data.  \n",
    "- Is it good or bad?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Note \n",
    ":class: note\n",
    "Although the plot above (complexity hyperparameter vs error) is more popular, we could also look at the same plot flip the $y$-axis, i.e., consider accuracy instead of error. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = np.arange(1, 18)\n",
    "accuracies = []\n",
    "for max_depth in max_depths:\n",
    "    accuracy = (\n",
    "        DecisionTreeClassifier(max_depth=max_depth).fit(X_subset, y).score(X_subset, y)\n",
    "    )\n",
    "    accuracies.append(accuracy)\n",
    "plt.plot(max_depths, accuracies)\n",
    "plt.xlabel(\"max depth\")\n",
    "plt.ylabel(\"accuracy\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ü§î Eva's questions\n",
    "\n",
    "![](img/eva-think.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At this point Eva is wondering about the following questions. \n",
    "\n",
    "- How to pick the best depth? \n",
    "- How can we make sure that the model we have built would do reasonably well on new data in the wild when it's deployed? \n",
    "- Which of the following rules learned by the decision tree algorithm are likely to generalize better to new data? \n",
    "\n",
    "> Rule 1: If class_attendance == 1 then grade is A+. \n",
    "\n",
    "> Rule 2: If lab3 > 83.5 and quiz1 <= 83.5 and lab2 <= 88 then quiz2 grade is A+\n",
    "\n",
    "To better understand the material in the next sections, think about these questions on your own or discuss them with your friend/neighbour before proceeding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalization: Fundamental goal of ML\n",
    "\n",
    "> **To generalize beyond what we see in the training examples**\n",
    "\n",
    "We only have access to limited amount of training data and we want to learn a mapping function which would predict targets reasonably well for examples beyond this training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Example: Imagine that a learner sees the following images and corresponding labels. \n",
    "\n",
    "![](img/generalization-train.png)\n",
    "<!-- <center>\n",
    "<img src='img/generalization-train.png' width=\"600\" height=\"600\" />\n",
    "</center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Generalizing to unseen data\n",
    "\n",
    "- Now the learner is presented with new images (1 to 4) for prediction. \n",
    "- What prediction would you expect for each image?   \n",
    "\n",
    "![](img/generalization-predict.png)\n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/generalization-predict.png' width=\"1000\" height=\"1000\" />\n",
    "</center>     -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Goal: We want the learner to be able to generalize beyond what it has seen in the training data.\n",
    "- But these new examples should be representative of the training data. That is they should have the same characteristics as the training data. \n",
    "- In this example, we would like the leaner to be able to predict labels for test examples 1, 2, and 3 accurately. Although 2, 3 don't exactly occur in the training data, they are very much similar to the images in the training data. That said, is it fair to expect the learner to label image 4 correctly? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Training error vs. Generalization error \n",
    "\n",
    "- Given a model $M$, in ML, people usually talk about two kinds of errors of $M$. \n",
    "    1. Error on the training data: $error_{training}(M)$ \n",
    "    2. Error on the entire distribution $D$ of data: $error_{D}(M)$\n",
    "- We are interested in the error on the entire distribution     \n",
    "    - ... But we do not have access to the entire distribution üòû"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Splitting "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to approximate generalization error? \n",
    "\n",
    "A common way is **data splitting**. \n",
    "- Keep aside some randomly selected portion from the training data.\n",
    "- `fit` (train) a model on the training portion only. \n",
    "- `score` (assess) the trained model on this set aside data to get a sense of how well the model would be able to generalize.\n",
    "- Pretend that the kept aside data is representative of the real distribution $D$ of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- We can pass `X` and `y` or a dataframe with both `X` and `y` in it. \n",
    "- We can also specify the train or test split sizes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Simple train/test split \n",
    "\n",
    "- The picture shows an 80%-20% split of a toy dataset with 10 examples. \n",
    "- The data is shuffled before splitting. \n",
    "- Usually when we do machine learning we split the data before doing anything and put the test data in an imaginary chest lock. \n",
    "\n",
    "![](img/train-test-split.png)\n",
    "\n",
    "<!-- <img src='img/train-test-split.png' width=\"1500\" height=\"1500\" /> -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's demonstrate this with the canada usa cities data\n",
    "# The data is available in the data directory\n",
    "df = pd.read_csv(\"data/canada_usa_cities.csv\")\n",
    "X = df.drop(columns=[\"country\"])\n",
    "y = df[\"country\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=123\n",
    ")  # 80%-20% train test split on X and y\n",
    "\n",
    "# Print shapes\n",
    "shape_dict = {\n",
    "    \"Data portion\": [\"X\", \"y\", \"X_train\", \"y_train\", \"X_test\", \"y_test\"],\n",
    "    \"Shape\": [\n",
    "        X.shape,\n",
    "        y.shape,\n",
    "        X_train.shape,\n",
    "        y_train.shape,\n",
    "        X_test.shape,\n",
    "        y_test.shape,\n",
    "    ],\n",
    "}\n",
    "\n",
    "shape_df = pd.DataFrame(shape_dict)\n",
    "HTML(shape_df.to_html(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Creating `train_df` and `test_df`\n",
    "\n",
    "- Sometimes we want to keep the target in the train split for EDA or for visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(\n",
    "    df, test_size=0.2, random_state=123\n",
    ")  # 80%-20% train test split on df\n",
    "X_train, y_train = train_df.drop(columns=[\"country\"]), train_df[\"country\"]\n",
    "X_test, y_test = test_df.drop(columns=[\"country\"]), test_df[\"country\"]\n",
    "print(train_df.head())\n",
    "print()\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.discrete_scatter(X.iloc[:, 0], X.iloc[:, 1], y, s=12)\n",
    "plt.xlabel(\"longitude\")\n",
    "plt.ylabel(\"latitude\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(X_train, y_train)\n",
    "display_tree(X_train.columns, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine the train and test accuracies with the split now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train accuracy:   %0.3f\" % model.score(X_train, y_train))\n",
    "print(\"Test accuracy:   %0.3f\" % model.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_tree_decision_boundary_and_tree(model, X, y, height=6, width=16, eps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6), subplot_kw={\"xticks\": (), \"yticks\": ()})\n",
    "plot_tree_decision_boundary(\n",
    "    model,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    eps=10,\n",
    "    x_label=\"longitude\",\n",
    "    y_label=\"latitude\",\n",
    "    ax=ax[0],\n",
    "    title=\"Decision tree model on the train data\",\n",
    ")\n",
    "plot_tree_decision_boundary(\n",
    "    model,\n",
    "    X_test,\n",
    "    y_test,\n",
    "    eps=10,\n",
    "    x_label=\"longitude\",\n",
    "    y_label=\"latitude\",\n",
    "    ax=ax[1],\n",
    "    title=\"Decision tree model on the test data\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Useful arguments of `train_test_split`: \n",
    "    - `test_size`\n",
    "    - `train_size` \n",
    "    - `random_state`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `test_size`, `train_size` arguments\n",
    "\n",
    "- Let's us specify how we want to split the data. \n",
    "- We can specify either of the two. See the documentation [here](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
    "- There is no hard and fast rule on what split sizes should we use. \n",
    "    - It depends upon how much data is available to you. \n",
    "- Some common splits are 90/10, 80/20, 70/30 (training/test).\n",
    "- In the above example, we used 80/20 split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `random_state` argument\n",
    "\n",
    "- The data is shuffled before splitting which is crucial step. (You will explore this in the lab.) \n",
    "- The `random_state` argument controls this shuffling. \n",
    "- In the example above we used `random_state=123`. If you run this notebook with the same `random_state` it should give you exactly the same split. \n",
    "    - Useful when you want reproducible results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/validation/test split\n",
    "\n",
    "- Some of you may have heard of \"validation\" data.\n",
    "- Sometimes it's a good idea to have a separate data for hyperparameter tuning. \n",
    "\n",
    "![](img/train-valid-test-split.png)\n",
    "\n",
    "<!-- <img src='img/train-valid-test-split.png' width=\"1500\" height=\"1500\" /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- We will try to use \"validation\" to refer to data where we have access to the target values.\n",
    "  - But, unlike the training data, we only use this for hyperparameter tuning and model assessment; we don't pass these into `fit`.  \n",
    "- We will try to use \"test\" to refer to data where we have access to the target values \n",
    "  - But, unlike training and validation data, we neither use it in training nor hyperparameter optimization. \n",
    "  - We only use it **once** to evaluate the performance of the best performing model on the validation set.   \n",
    "  - We lock it in a \"vault\" until we're ready to evaluate. \n",
    "\n",
    "Note that there isn't good concensus on the terminology of what is validation and what is test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Note \n",
    ":class: note\n",
    "Validation data is also referred to as **development data** or **dev set** for short.  \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### \"Deployment\" data\n",
    "\n",
    "- After we build and finalize a model, we deploy it, and then the model deals with the data in the wild. \n",
    "- We will use \"deployment\" to refer to this data, where we do **not** have access to the target values.\n",
    "- Deployment error is what we _really_ care about.\n",
    "- We use validation and test errors as proxies for deployment error, and we hope they are similar.\n",
    "- So, if our model does well on the validation and test data, we hope it will do well on deployment data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Summary of train, validation, test, and deployment data\n",
    "\n",
    "|         | `fit` | `score` | `predict` |\n",
    "|----------|-------|---------|-----------|\n",
    "| Train    | ‚úîÔ∏è      | ‚úîÔ∏è      | ‚úîÔ∏è         |\n",
    "| Validation |      | ‚úîÔ∏è      | ‚úîÔ∏è         |\n",
    "| Test    |       |  once   | once         |\n",
    "| Deployment    |       |       | ‚úîÔ∏è         |\n",
    "\n",
    "You can typically expect $E_{train} < E_{validation} < E_{test} < E_{deployment}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ‚ùì‚ùì Questions on generalization and data splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{admonition} Exercise 3.1: True or False\n",
    "1. A decision tree model with no depth is likely to perform very well on the deployment data. \n",
    "2. Data splitting helps us generalize our model better. \n",
    "3. Deployment data is used at the very end and only scored once.  \n",
    "4. Validation data could be used for hyperparameter optimization. \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "```{admonition} Exercise 3.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1. False\n",
    "2. False. Data splitting helps us **assess** how well our model would generalize. \n",
    "3. False. You cannot score on the deployment data as there are no labels available.\n",
    "4. True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "```{admonition} Exercise 3.2: Questions for discussion\n",
    "1. Why you can typically expect $E_{train} < E_{validation} < E_{test} < E_{deployment}$. (From Piazza.)\n",
    "2. Discuss the consequences of not shuffling before splitting the data in `train_test_split`. \n",
    "```    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross-validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problems with single train/validation split\n",
    "\n",
    "- Only using a portion of your data for training and only a portion for validation.\n",
    "- If your dataset is small you might end up with a tiny training and/or validation set.\n",
    "- You might be unlucky with your splits such that they don't align well or don't well represent your test data.\n",
    "\n",
    "![](img/train-valid-test-split.png)\n",
    "\n",
    "<!-- <img src='img/train-valid-test-split.png' width=\"1500\" height=\"1500\" /> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation to the rescue!! \n",
    "\n",
    "- Cross-validation provides a solution to this problem. \n",
    "- Split the data into $k$ folds ($k>2$, often $k=10$). In the picture below $k=4$.\n",
    "- Each \"fold\" gets a turn at being the validation set.\n",
    "- Note that cross-validation doesn't shuffle the data; it's done in `train_test_split`.\n",
    "\n",
    "![](img/cross-validation.png)\n",
    "<!-- <img src='img/cross-validation.png' width=\"1500\"> -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Each fold gives a score and we usually average our $k$ results. \n",
    "- It's better to examine the variation in the scores across folds.  \n",
    "- Gives a more **robust** measure of error on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Cross-validation using `scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `cross_val_score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=4)\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average cross-validation score = {np.mean(cv_scores):.2f}\")\n",
    "print(f\"Standard deviation of cross-validation score = {np.std(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Under the hood\n",
    "\n",
    "- It creates `cv` folds on the data.\n",
    "- In each fold, it fits the model on the training portion and scores on the validation portion. \n",
    "- The output is a list of validation scores in each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### `cross_validate`\n",
    "\n",
    "- Similar to `cross_val_score` but more powerful.\n",
    "- Let's us access training and validation scores.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pd.DataFrame(scores).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{important}\n",
    "Keep in mind that cross-validation does not return a model. It is not a way to build a model that can be applied to new data. The purpose of cross-validation is to **evaluate** how well the model will generalize to unseen data. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{seealso}\n",
    "Note that both `cross_val_score` and `cross_validate` functions do not shuffle the data. Check out [`StratifiedKFold`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedKFold.html#sklearn.model_selection.StratifiedKFold), where proportions of classes is the same in each fold as they are in the whole dataset. By default, `sklearn` uses `StratifiedKFold` when carrying out cross-validation for classification problems. \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_cross_validation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Our typical supervised learning set up is as follows: \n",
    "\n",
    "- We are given training data with features `X` and target `y`\n",
    "- We split the data into train and test portions: `X_train, y_train, X_test, y_test`\n",
    "- We carry out hyperparameter optimization using cross-validation on the train portion: `X_train` and `y_train`. \n",
    "- We assess our best performing model on the test portion: `X_test` and `y_test`.  \n",
    "- What we care about is the **test error**, which tells us how well our model can be generalized.\n",
    "- If this test error is \"reasonable\" we deploy the model which will be used on new unseen examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "model = DecisionTreeClassifier(max_depth=10)\n",
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "pd.DataFrame(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n",
    "    \"\"\"\n",
    "    Returns mean and std of cross validation\n",
    "    \"\"\"\n",
    "    scores = cross_validate(model, X_train, y_train, **kwargs)\n",
    "\n",
    "    mean_scores = pd.DataFrame(scores).mean()\n",
    "    std_scores = pd.DataFrame(scores).std()\n",
    "    out_col = []\n",
    "\n",
    "    for i in range(len(mean_scores)):\n",
    "        out_col.append((f\"%0.3f (+/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n",
    "\n",
    "    return pd.Series(data=out_col, index=mean_scores.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "results[\"Decision tree\"] = mean_std_cross_val_scores(\n",
    "    model, X_train, y_train, return_train_score=True\n",
    ")\n",
    "pd.DataFrame(results).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How do we know whether this test score is reasonable? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ‚ùì‚ùì Questions on cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Exercise 3.3: Cross-validation\n",
    "1. $k$-fold cross-validation calls fit $k$. True or False? \n",
    "2. We use cross-validation to improve model performance. True or False? \n",
    "3. Discuss advantages and disadvantages of cross-validation. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Exercise 3.3: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1. True \n",
    "2. False. We can use it to assess model performance.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting, overfitting, the fundamental trade-off, the golden rule "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of errors\n",
    "\n",
    "Imagine that your train and validation errors do not align with each other. How do you diagnose the problem?  \n",
    "\n",
    "We're going to think about 4 types of errors:\n",
    "\n",
    "- $E_\\textrm{train}$ is your training error (or mean train error from cross-validation).\n",
    "- $E_\\textrm{valid}$ is your validation error (or mean validation error from cross-validation).\n",
    "- $E_\\textrm{test}$ is your test error.\n",
    "- $E_\\textrm{best}$ is the best possible error you could get for a given problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Underfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=1)  # decision stump\n",
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "print(\"Train error:   %0.3f\" % (1 - np.mean(scores[\"train_score\"])))\n",
    "print(\"Validation error:   %0.3f\" % (1 - np.mean(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If your model is too simple, like `DummyClassifier` or `DecisionTreeClassifier` with `max_depth=1`, it's not going to pick up on some random quirks in the data but it won't even capture useful patterns in the training data.\n",
    "- The model won't be very good in general. Both train and validation errors would be high. This is **underfitting**.\n",
    "- The gap between train and validation error is going to be lower.\n",
    "- $E_\\textrm{best} \\lt E_\\textrm{train} \\lesssim E_\\textrm{valid}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overfitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=None)\n",
    "scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "print(\"Train error:   %0.3f\" % (1 - np.mean(scores[\"train_score\"])))\n",
    "print(\"Validation error:   %0.3f\" % (1 - np.mean(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- If your model is very complex, like a `DecisionTreeClassifier(max_depth=None)`, then you will learn unreliable patterns in order to get every single training example correct.\n",
    "- The training error is going to be very low but there will be a big gap between the training error and the validation error. This is **overfitting**.\n",
    "- In overfitting scenario, usually we'll see: \n",
    "$E_\\textrm{train} \\lt E_\\textrm{best}  \\lt E_\\textrm{valid}$\n",
    "- In general, if $E_\\textrm{train}$ is low, we are likely to be in the overfitting scenario. It is fairly common to have at least a bit of this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- So the validation error does not necessarily decrease with the training error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "results_dict = {\n",
    "    \"depth\": [],\n",
    "    \"mean_train_error\": [],\n",
    "    \"mean_cv_error\": [],\n",
    "    \"std_cv_error\": [],\n",
    "    \"std_train_error\": [],\n",
    "}\n",
    "param_grid = {\"max_depth\": np.arange(1, 16)}\n",
    "\n",
    "for depth in param_grid[\"max_depth\"]:\n",
    "    model = DecisionTreeClassifier(max_depth=depth)\n",
    "    scores = cross_validate(model, X_train, y_train, cv=10, return_train_score=True)\n",
    "    results_dict[\"depth\"].append(depth)\n",
    "    results_dict[\"mean_cv_error\"].append(1 - np.mean(scores[\"test_score\"]))\n",
    "    results_dict[\"mean_train_error\"].append(1 - np.mean(scores[\"train_score\"]))\n",
    "    results_dict[\"std_cv_error\"].append(scores[\"test_score\"].std())\n",
    "    results_dict[\"std_train_error\"].append(scores[\"train_score\"].std())\n",
    "\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "results_df = results_df.set_index(\"depth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "results_df[[\"mean_train_error\", \"mean_cv_error\"]].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Here, for larger depths we observe that the training error is close to 0 but validation error goes up and down. \n",
    "- As we make more complex models we start encoding random quirks in the data, which are not grounded in reality.  \n",
    "- These random quirks do not generalize well to new data. \n",
    "- This problem of failing to be able to generalize to the validation data or test data is called **overfitting**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The \"fundamental tradeoff\" of supervised learning:\n",
    "\n",
    "\n",
    "**As you increase model complexity, $E_\\textrm{train}$ tends to go down but $E_\\textrm{valid}-E_\\textrm{train}$ tends to go up.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bias vs variance tradeoff \n",
    "\n",
    "- The fundamental trade-off is also called the bias/variance tradeoff in supervised machine learning.\n",
    "\n",
    "**Bias**\n",
    ": the tendency to consistently learn the same wrong thing (high bias corresponds to underfitting)\n",
    "\n",
    "**Variance** \n",
    ": the tendency to learn random things irrespective of the real signal (high variance corresponds to overfitting)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{seealso} \n",
    "Check out [this article by Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) for some approachable explanation on machine learning fundamentals and bias-variance tradeoff. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How to pick a model that would generalize better?\n",
    "\n",
    "- We want to avoid both underfitting and overfitting. \n",
    "- We want to be consistent with the training data but we don't to rely too much on it. \n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/malp_0201.png' width=\"800\" height=\"800\" />\n",
    "</center>    \n",
    " -->\n",
    "![](img/malp_0201.png)\n",
    "\n",
    "[source](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch02.html#relation-of-model-complexity-to-dataset-size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- There are many subtleties here and there is no perfect answer but a  common practice is to pick the model with minimum cross-validation error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validate_std(*args, **kwargs):\n",
    "    \"\"\"Like cross_validate, except also gives the standard deviation of the score\"\"\"\n",
    "    res = pd.DataFrame(cross_validate(*args, **kwargs))\n",
    "    res_mean = res.mean()\n",
    "    res_mean[\"std_test_score\"] = res[\"test_score\"].std()\n",
    "    if \"train_score\" in res:\n",
    "        res_mean[\"std_train_score\"] = res[\"train_score\"].std()\n",
    "    return res_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### test score vs. cross-validation score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "best_depth = results_df.index.values[np.argmin(results_df[\"mean_cv_error\"])]\n",
    "print(\n",
    "    \"The minimum validation error is %0.3f at max_depth = %d \"\n",
    "    % (\n",
    "        np.min(results_df[\"mean_cv_error\"]),\n",
    "        best_depth,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's pick `max_depth`= 5 and try this model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(max_depth=best_depth)\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"Error on test set: {1 - model.score(X_test, y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The test error is comparable with the cross-validation error. \n",
    "- Do we feel confident that this model would give similar performace when deployed? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The golden rule <a name=\"4\"></a>\n",
    "\n",
    "- Even though we care the most about test error **THE TEST DATA CANNOT INFLUENCE THE TRAINING PHASE IN ANY WAY**. \n",
    "- We have to be very careful not to violate it while developing our ML pipeline. \n",
    "- Even experts end up breaking it sometimes which leads to misleading results and lack of generalization on the real data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Golden rule violation: Example 1  \n",
    "\n",
    "![](img/golden_rule_violation.png)\n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/golden_rule_violation.png' width=\"500\" height=\"500\" />\n",
    "</center>    \n",
    " -->\n",
    " \n",
    "<blockquote>\n",
    "   ... He attempted to reproduce the research, and found a major flaw: there was some overlap in the data used to both train and test the model. \n",
    "</blockquote>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Golden rule violation: Example 2  \n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/golden_rule_violation_2.png' width=\"500\" height=\"500\" />\n",
    "</center>    \n",
    " -->\n",
    " \n",
    "![](img/golden_rule_violation_2.png)\n",
    "\n",
    "<blockquote>\n",
    "  ... The Challenge rules state that you must only test your code twice a week, because there‚Äôs an element of chance to the results. Baidu has admitted that it used multiple email accounts to test its code roughly 200 times in just under six months ‚Äì over four times what the rules allow.\n",
    "</blockquote>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How can we avoid violating golden rule? \n",
    "\n",
    "- Recall that when we split data, we put our test set in an imaginary vault.\n",
    "\n",
    "<!-- <center>\n",
    "<img src='img/train-test-split.png' width=\"1500\" height=\"1500\" />\n",
    "</center>    \n",
    " -->\n",
    " \n",
    "![](img/train-test-split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Here is the workflow we'll generally follow. \n",
    "\n",
    "- **Splitting**: Before doing anything, split the data `X` and `y` into `X_train`, `X_test`, `y_train`, `y_test` or `train_df` and `test_df` using `train_test_split`. \n",
    "- **Select the best model using cross-validation**: Use `cross_validate` with `return_train_score = True` so that we can get access to training scores in each fold. (If we want to plot train vs validation error plots, for instance.) \n",
    "- **Scoring on test data**: Finally score on the test data with the chosen hyperparameters to examine the generalization performance.\n",
    "\n",
    "**Again, there are many subtleties here we'll discuss the golden rule multiple times throughout the course and in the program.**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### ‚ùì‚ùì Questions for you"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Exercise 3.4\n",
    "\n",
    "Underfitting or overfitting? \n",
    "1. If the mean train accuracy is much higher than the mean cross-validation accuracy.\n",
    "2. If the mean train accuracy and the mean cross-validation accuracy are both low and relatively similar in value.\n",
    "3. Decision tree with no limit on the depth. \n",
    "4. Decision stump on a complicated classification problem. \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Exercise 3.4: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1. Overfitting\n",
    "2. Underfitting\n",
    "3. Overfitting\n",
    "4. Underfitting\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Exercise 3.5\n",
    " \n",
    "State whether True/False. \n",
    "\n",
    "1. In supervised learning, the training error is always lower than the validation error.\n",
    "2. The fundamental tradeoff of ML states that as training error goes down, validation error goes up.\n",
    "3. More \"complicated\" models are more likely to overfit than \"simple\" ones.\n",
    "4. If we had an infinite amount of training data, overfitting would not be a problem.\n",
    "5. If our training error is extremely low, we are likely to be overfitting.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "```{admonition} Exercise 3.5: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "1. False\n",
    "2. False\n",
    "3. True\n",
    "4. True\n",
    "5. True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What did we learn today? \n",
    "\n",
    "- Importance of generalization in supervised machine learning \n",
    "- Data splitting as a way to approximate generalization error \n",
    "- Train, test, validation, deployment data\n",
    "- Cross-validation\n",
    "- A typical sequence of steps to train supervised machine learning models\n",
    "    - training the model on the train split\n",
    "    - tuning hyperparamters using the validation split\n",
    "    - checking the generalization performance on the test split \n",
    "- Overfitting, underfitting, the fundamental tradeoff, and the golden rule.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coming up ...  \n",
    "\n",
    "- KNNs, SVM RBFs \n",
    "- Preprocessing\n",
    "    - Imputation\n",
    "    - Scaling\n",
    "    - One-hot encoding\n",
    "    - `sklearn` pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-seeyou.png)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
